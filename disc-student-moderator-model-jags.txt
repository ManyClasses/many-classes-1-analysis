### data dictionary
# delta.z[s] is the difference in z-scores for individual student s (immediate - delayed)
# n.students is the number of unique students
# class.index[s] is student s's class ID
# class.condition[c] is classroom c's incentive condition
# n.classes is the number of unique classes
# n.conditions is the number of unique conditions that split all the moderators (here it will be 2 for no incentive and yes incentive)

### parameter dictionary
# mu.class[c] is the mean of the distribution from which delta.z scores are drawn for class c
# sigma.class[c] is the sd of the distribution from which delta.z scores are drawn for class c
# sigma.mode is the mode of the distribution from which classroom sigma parameters are drawn 
# sigma.sd is the sd of the distribution from which classroom sigma parameters are drawn
# mu.condition.effect[i] is the mean of the distribution from which mu.class values are drawn in condition i
# sigma.condition.effect[i] is the sd of the distribution from which mu.class values are drawn in condition i
# mu.condition.diff is simple mu.condition.effect[1] - mu.condition.effect[2]. this is the difference between incentive and no incentive conditions.

model {
  for(s in 1:n.students){
    # student performance is sampled from a normal distribution that is specific to each class
    delta.z[s] ~ dnorm(mu[s], 1 / sigma.class[class.index[s]] ^ 2)
    mu[s] <- mu.class[class.index[s]] + moderator.effect[class.condition[class.index[s]], moderator.level[s]]
  }
  for(c in 1:n.classes){
    # the mean of the class distribution is sampled from a condition-specific normal, with moderator effect.
    mu.class[c] ~ dnorm(mu.condition.effect[class.condition[c]] , 1 / sigma.condition.effect[class.condition[c]] ^ 2)
    
    # the sd of the class distribution is sampled from a gamma
    sigma.class[c] ~ dgamma(sh.sigma.class, ra.sigma.class)
  }
  for(i in 1:n.conditions){
    # prior distribution for the condition-specific mu normal
    mu.condition.effect[i] ~ dnorm(0,1)
    sigma.condition.effect[i] ~ dgamma(1.640388, 1.280776) # mode = 0.5, sd = 1
    for(l in 1:n.moderator.levels){
      moderator.effect[i,l] ~ dnorm(0, 1 / moderator.sd[i]^2)
    }
    moderator.sd[i] ~ dgamma(1.640388, 1.280776) # mode = 0.5, sd = 1; this is probably not wide enough?
  }
  # prior distribution for the gamma defining sd of classes
  ra.sigma.class <- (sigma.mode + sqrt(sigma.mode^2 + 4*sigma.sd^2)) / (2 * sigma.sd^2)
  sh.sigma.class <- 1 + sigma.mode * ra.sigma.class
  
  sigma.mode ~ dgamma(1.640388, 1.280776) # mode = 0.5, sd = 1
  sigma.sd ~ dgamma(1.640388, 1.280776) # mode = 0.5, sd = 1
  
  # computed bayesian r2 (http://www.stat.columbia.edu/~gelman/research/unpublished/bayes_R2.pdf)
  residuals <- mu.class - mu
  var.predict <- sd(mu) ^ 2
  var.residuals <- sd(residuals) ^ 2
  s <- var.predict + var.residuals
  r.sq <- var.predict / (s+0.000000000001)
  
}